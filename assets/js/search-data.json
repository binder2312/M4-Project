{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": "SDS 2020 - Module 3: Individual Assignment . import pandas as pd import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential from keras.preprocessing.text import Tokenizer from sklearn.model_selection import train_test_split from tensorflow.keras.preprocessing.sequence import pad_sequences import matplotlib.pyplot as plt . Data and preprocessing . tweets = pd.read_json(&quot;https://github.com/SDS-AAU/SDS-master/raw/e2c959494d53859c1844604bed09a28a21566d0f/M3/assignments/trump_vs_GPT2.gz&quot;) . tweets.columns = [&quot;text&quot;, &quot;status&quot;] . tweets[&#39;status&#39;].astype(int) . 0 1 1 1 2 1 3 1 4 1 .. 14731 0 14732 0 14733 0 14734 0 14735 0 Name: status, Length: 14736, dtype: int64 . length = [] [length.append(len(str(text))) for text in tweets[&#39;text&#39;]] tweets[&#39;length&#39;] = length tweets.head() . text status length . 0 I was thrilled to be back in the Great city of... | True | 210 | . 1 The Unsolicited Mail In Ballot Scam is a major... | True | 100 | . 2 As long as I am President, I will always stand... | True | 82 | . 3 Our Economy is doing great, and is ready to se... | True | 81 | . 4 If I do not sound like a typical Washington po... | True | 90 | . print(min(tweets[&#39;length&#39;])) print(max(tweets[&#39;length&#39;])) print(round(sum(tweets[&#39;length&#39;])/len(tweets[&#39;length&#39;]))) . 0 296 163 . # I&#39;m choosing this since i want a full sentence and not just single words len(tweets[tweets[&#39;length&#39;] &lt; 10]) . 408 . tweets = tweets[tweets.length &gt;= 10] . # any unknown words will be replaced # Also removing punctuation by default tokenizer = Tokenizer(lower=True, oov_token=True) . # I want to predict the status 0 or 1, so that will be the dependent variable X = tweets[&#39;text&#39;] y = tweets[&#39;status&#39;] . # Picking a test size of 0.3 - tried to adjust it, but result didn&#39;t get better X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . tokenizer.fit_on_texts(X_train) . # Here i have the size for the input_dim in the Embedding layer max_vocab = len(tokenizer.word_index)+1 . sequences_train = tokenizer.texts_to_sequences(X_train) sequences_test = tokenizer.texts_to_sequences(X_test) . # This is required when giving input to the neural net data_train = pad_sequences(sequences_train) data_test = pad_sequences(sequences_test, maxlen=data_train.shape[1]) . Building the model . model = keras.Sequential() . # Where the input size is equal to the vocabulary size + 1 # Keeping a small number for output since it gave the best results model.add(layers.Embedding(max_vocab, 16)) . # Where the normal LSTM model only reads from left to right # So this one should give a better result than the normal one # Also adding Dropout of 0.9 since i found that the model performed better with this model.add(layers.Bidirectional(layers.LSTM(8, return_sequences=True, dropout=0.9))) . model.add(layers.Dense(1, activation=&#39;sigmoid&#39;)) . # I found that when trying with over 1 million parameters the model gets overfitted real quick model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, None, 16) 209920 _________________________________________________________________ bidirectional (Bidirectional (None, None, 16) 1600 _________________________________________________________________ dense (Dense) (None, None, 1) 17 ================================================================= Total params: 211,537 Trainable params: 211,537 Non-trainable params: 0 _________________________________________________________________ . Training and evaluation of the model . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . # I have tried many different setups, and in many cases i ended up overfitting the training set r = model.fit(data_train, y_train, epochs=6, validation_data=(data_test, y_test), batch_size=64) . Epoch 1/6 157/157 [==============================] - 3s 19ms/step - loss: 0.6148 - accuracy: 0.6818 - val_loss: 0.4837 - val_accuracy: 0.8163 Epoch 2/6 157/157 [==============================] - 2s 15ms/step - loss: 0.4795 - accuracy: 0.8043 - val_loss: 0.4379 - val_accuracy: 0.8270 Epoch 3/6 157/157 [==============================] - 2s 15ms/step - loss: 0.4384 - accuracy: 0.8196 - val_loss: 0.3953 - val_accuracy: 0.8462 Epoch 4/6 157/157 [==============================] - 2s 15ms/step - loss: 0.4003 - accuracy: 0.8390 - val_loss: 0.3772 - val_accuracy: 0.8491 Epoch 5/6 157/157 [==============================] - 2s 15ms/step - loss: 0.3732 - accuracy: 0.8518 - val_loss: 0.3602 - val_accuracy: 0.8547 Epoch 6/6 157/157 [==============================] - 2s 15ms/step - loss: 0.3514 - accuracy: 0.8588 - val_loss: 0.3466 - val_accuracy: 0.8591 . I can see here that the loss score is decreasing for both the training and test set in each epoch as well as the accuracy is increasing which is the goal for this step here where i want to achieve a low loss and a high accuracy meaning that the model is as precise as it can be. The reason why i didn&#39;t choose more epochs is because the training set starts to get overfitted from here. PLEASE BE AWARE THAT THE val_accuracy IS FOUND UNDER THE &#39;Epoch 1/6 157/157&#39;. . plt.plot(r.history[&#39;accuracy&#39;]) plt.plot(r.history[&#39;val_accuracy&#39;]) plt.title(&#39;Model Accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;, labelpad=2) plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;) plt.show() . From this i can see that both the train and test set is improving over the 6 epochs and then ending up being approximately the same in the last epoch. (0, 1, 2, 3, 4, 5 - so six) . plt.plot(r.history[&#39;loss&#39;]) plt.plot(r.history[&#39;val_loss&#39;]) plt.title(&#39;Loss&#39;) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper right&#39;) plt.show() . The same is the case for the loss score, where the graphs are almost meeting each other in the 6th epoch which means that it is the near perfect fit. . I had a lot of trouble finding a good match, where in many cases the model ended up being overfit - being that the training set was improving really quick ending on 0.98 on the accuracy score but with high loss score on the validation set - which should be as low as possible. I have both tried adding more layers to the model and also adjusting the number of neurons in the layers, but i found that keeping the model on few layers with few neurons and a high dropout rate gave me the best performing neural net. . So this means that the model will be able to detect a fake tweet with an accuracy of 86%. . !jupyter nbconvert --to html &quot;/content/M3A1.ipynb&quot; .",
            "url": "https://binder2312.github.io/M4-Project/fastpages/jupyter/2021/01/06/Individual-Assignment.html",
            "relUrl": "/fastpages/jupyter/2021/01/06/Individual-Assignment.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://binder2312.github.io/M4-Project/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "We are a group of four students from Aalborg University studying Social Data Science for our 9. semester. .",
          "url": "https://binder2312.github.io/M4-Project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://binder2312.github.io/M4-Project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}